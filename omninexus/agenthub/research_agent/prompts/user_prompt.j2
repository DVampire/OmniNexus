<IMPORTANT>

* When generate research project, you MUST follow the following steps:
    1. You MUST follow the PEP8 standard for the generated python code.
    2. Before generating the modules:
        * You MUST first design the code structure of the research project according to the template.
        * You MUST create the necessary directories for a research project (e.g., src, configs, datasets, tests, src/config/, src/data/, etc.) before generating the modules.
    3. Generating the modules:
        * You MUST generate each module one by one and create the necessary Python scripts for each module in the src package (e.g., __init__.py and model.py in the model module).
    4. After generated the modules:
        * You MUST generate the necessary unit tests for each module in the tests package.
        * You MUST create the experiment parameters in the config file.
        * You MUST generate the main entry point for running the project (e.g., run.py).
        * You MUST generate the README, requirements.txt, setup.py, and other necessary files for the project.

* When browsing the latest research about some a paper or a topic, you MUST follow the following steps:
    1. You MUST first identify at least 5 key research directions or topics related to the paper or topic.
    2. For each research direction, you MUST search for at least 3 research papers.
    3. You MUST provide the base information of the research papers, including the TITLE, PAPER LINK, RESEARCH DIRECTION, DATE, number of CITATIONS, and ABSTRACT.
    4. You MUST read the abstract, introduction, and conclusion, limitations, and future work of the research papers. And then, you MUST summarize the CONTRIBUTIONS, LIMITATIONS and IMPROVEMENTS of the research papers.

    ** Examples **
    Paper 1:
        1. TITLE: Attention is All You Need
        2. PAPER LINK: https://arxiv.org/abs/1706.03762
        3. RESEARCH DIRECTION: Transformer Models
        4. DATE: June 2017
        5. CITATIONS: 145444
        6. ABSTRACT: The dominant sequence transduction models are based on complex recurrent or
                    convolutional neural networks that include an encoder and a decoder. The best
                    performing models also connect the encoder and decoder through an attention
                    mechanism. We propose a new simple network architecture, the Transformer,
                    based solely on attention mechanisms, dispensing with recurrence and convolutions
                    entirely. Experiments on two machine translation tasks show these models to
                    be superior in quality while being more parallelizable and requiring significantly
                    less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German
                    translation task, improving over the existing best results, including
                    ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
                    our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
                    training for 3.5 days on eight GPUs, a small fraction of the training costs of the
                    best models from the literature. We show that the Transformer generalizes well to
                    other tasks by applying it successfully to English constituency parsing both with
                    large and limited training data.
        7. CONTRIBUTIONS:
            - The paper introduces the Transformer, a novel architecture that relies entirely on self-attention mechanisms, removing the need for recurrence or convolution.
            - It significantly improves computational efficiency by enabling better parallelization and faster training on long sequences.
            - The multi-head attention mechanism is proposed to allow the model to attend to multiple positions and subspaces simultaneously.
            - Positional encodings are introduced to represent sequence order in the absence of recurrence.
            - The Transformer achieves state-of-the-art results on machine translation tasks, including WMT 2014 English-to-German and English-to-French benchmarks.
            - The model demonstrates that attention-based architectures can outperform traditional recurrent models while simplifying overall design.
        8. LIMITATIONS:
            - The self-attention mechanism has quadratic time and memory complexity, making it computationally expensive for long sequences.
            - The use of fixed positional encodings limits flexibility when handling sequences longer than those seen during training.
            - The Transformer requires large datasets and significant computational resources, which can be prohibitive for some users.
            - Unlike RNNs, the Transformer lacks an inherent sequential inductive bias, which may be a disadvantage for strict sequence modeling tasks.
            - The model struggles to capture long-range dependencies in extremely long sequences without additional modifications.
            - Its highly parameterized design can lead to overfitting and poor generalization on tasks with limited training data.
        9. IMPROVEMENTS:
            - Use sparse or approximate attention to reduce the quadratic complexity of self-attention for long sequences.
            - Replace fixed positional encodings with learnable or dynamic positional embeddings to enhance flexibility.
            - Incorporate mechanisms like sliding windows or memory-augmented modules to better handle long-range dependencies and extended contexts.
    Paper 2:
        1. TITLE: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
        2. PAPER LINK: https://arxiv.org/abs/1810.04805
        3. RESEARCH DIRECTION: Pre-trained Language Models
        4. DATE: October 2018
        5. CITATIONS: 121777

        6. ABSTRACT: We introduce BERT (Bidirectional Encoder Representations from Transformers), a new
                    method of pre-training language representations that achieves state-of-the-art results
                    on a wide range of natural language processing tasks. Unlike previous works, BERT is
                    deeply bidirectional, jointly conditioning on both left and right context in all layers,
                    which improves performance over unidirectional language models. BERT is pre-trained using
                    two unsupervised tasks: Masked Language Modeling (MLM), where random words are masked, and
                    the model predicts them, and Next Sentence Prediction (NSP), which helps the model understand
                    sentence relationships. BERT achieves new state-of-the-art results on 11 NLP tasks, including
                    the GLUE benchmark, SQuAD v1.1, and SQuAD v2.0, demonstrating its versatility and effectiveness.
        7. CONTRIBUTIONS:
            - Introduces BERT, a pre-trained bidirectional Transformer-based model that learns deep contextualized word representations.
            - Proposes the Masked Language Modeling (MLM) task to enable bidirectional pre-training of Transformers by randomly masking input tokens.
            - Develops the Next Sentence Prediction (NSP) task to help the model capture sentence-level relationships for downstream tasks.
            - Demonstrates the effectiveness of BERT on diverse NLP tasks, achieving state-of-the-art performance on GLUE, SQuAD v1.1, and SQuAD v2.0 benchmarks.
            - Shows that pre-training a deep Transformer on a large text corpus can be fine-tuned to achieve exceptional results on specific tasks with minimal modifications.
            - Establishes the pre-train, fine-tune paradigm, which has become a foundation for modern NLP research.
        8. LIMITATIONS:
            - BERT’s quadratic complexity in self-attention limits its scalability to very long sequences.
            - The masked token prediction leads to a mismatch between pre-training and fine-tuning, as the model does not encounter masked tokens during downstream tasks.
            - Training BERT requires significant computational resources, making it less accessible to smaller research groups.
            - BERT does not handle long-term dependencies efficiently, as it has a fixed input length (e.g., 512 tokens).
            - The model’s fine-tuning process can be unstable for certain tasks, requiring careful hyperparameter tuning.
            - BERT’s bidirectional nature may lead to information leakage when applied to autoregressive tasks like language generation.
        9. IMPROVEMENTS:
            - Optimize self-attention mechanisms using sparse attention or sliding windows to handle longer sequences efficiently.
            - Address pre-training and fine-tuning mismatch by introducing more realistic masking strategies or dynamic masking during fine-tuning.
            - Develop lightweight or distilled versions of BERT (e.g., DistilBERT) to reduce computational costs while maintaining performance.

    Summary of the above papers:
        1. RESEARCH DIRECTIONS:
            - Transformer Models: Focus on architectures based on self-attention mechanisms like the Transformer, enabling parallelization and efficient sequence processing.
            - Pre-trained Language Models: Explore methods for pre-training deep contextualized representations like BERT, leveraging large text corpora for transfer learning.
        2. CHALLENGES:
            - Computational Complexity: Models like the Transformer and BERT face challenges with quadratic self-attention complexity, limiting scalability to long sequences.
            - Pre-training and Fine-tuning Mismatch: Mismatches between pre-training objectives (e.g., MLM) and downstream tasks can hinder model performance and generalization.
        3. IMPROVEMENTS:
            - Exploring more efficient and scalable pre-training methods for large language models.
            - Investigating ways to improve the interpretability and explainability of deep learning models.

* When provide some novel ideas related to a paper or a topic, you MUST follow the following steps:
    1. You MUST first identify the RESEARCH DOMAIN or research field you are addressing, such as Large Language Models (LLMs), Reinforcement Learning and Human Feedback (RLHF), or Multimodal Learning.
    2. You MUST provide a brief overview of the RESEARCH DIRECTIONS or topics within the domain, highlighting key areas of focus and recent advancements.
    3. You MUST identify the key CHALLENGES or limitations faced by the research field or the specific paper or topic you are addressing. You can refer to existing research papers, reviews, or discussions to identify these challenges.
    4. You MUST brainstorm new ideas or approaches that address the identified directions and challenges. These ideas should be innovative, feasible, and have the potential to advance the research field.
    5. You MUST describe how to implement the novel ideas in the research project for each idea. You can provide a high-level overview of the proposed methods, algorithms, or architectures.

    ** Examples **
    * RESEARCH DOMAIN: Large Language Models (LLMs)
    * RESEARCH DIRECTIONS:
        - Efficiency and Resource Optimization: Research focuses on reducing computational costs using techniques like model quantization, pruning, and distillation.
        - Alignment with Human Intent: Advances in RLHF and preference modeling aim to improve LLMs' ability to align with user intent while minimizing harmful outputs.
        - Multimodal Capabilities: LLMs are increasingly integrated with other modalities, enabling models like GPT-4 Vision to process text, images, and audio seamlessly.
        - Long-Context Understanding: Techniques like extended context windows and efficient attention mechanisms enhance LLMs' ability to process long documents and sequences.
        - Trustworthiness and Explainability: Efforts focus on improving LLMs' interpretability, mitigating biases, and ensuring safe, reliable outputs for critical applications.
        - Enhanced Reasoning: Models like GPT-o1 emphasize improving reasoning performance by enabling extended deliberation for complex tasks, such as scientific problem-solving and mathematical reasoning
    * CHALLENGES:
         - Computational Cost and Efficiency: Training and deploying LLMs require immense computational resources, leading to high costs and energy consumption, which hinder scalability and accessibility.
         - Hallucinations and Reliability: LLMs often generate hallucinated outputs (factually incorrect or fabricated information), posing risks in critical applications like healthcare and finance.
         - Alignment with Human Values: Ensuring that LLMs consistently align with human intent and ethical values remains challenging, especially in reducing biases, harmful content, and unintended behaviors.
         - Long-Context Limitations: Despite progress, LLMs still struggle with effectively processing and reasoning over long sequences of text, leading to loss of context or degradation in performance.
         - Data Privacy and Security: Training and fine-tuning LLMs on vast datasets raise concerns regarding the use of private or sensitive information, along with risks like model inversion and data leakage.
    * NOVEL IDEAS:
         Idea 1: Adaptive Computation for Dynamic Reasoning
            1. IDEA: Develop a multi-stage adaptive computation mechanism where LLMs selectively apply deeper layers and longer reasoning paths only when task complexity demands it. This could involve early stopping, dynamic depth control, or routing tasks to lightweight sub-networks.
            2. IMPLEMENTATION:
                - Task Complexity Estimation: Introduce a lightweight task complexity classifier that evaluates input queries based on entropy, token distribution, or confidence scores.
                - Dynamic Routing: Use a mixture-of-experts (MoE) approach or layer-wise gating mechanisms to activate specific parts of the model for complex tasks while skipping others for simpler ones.
                - Efficient Inference: Integrate adaptive early-exit techniques (e.g., depth-skip transformers) that allow stopping computation once confidence thresholds are met.
            3. ALGORITHM:
                - Input -> Task Complexity Estimator -> Adaptive Routing (select shallow or deep pathway) -> Output.
                - For complex tasks, deeper layers with chain-of-thought prompting are activated for multi-step reasoning.
         Idea 2: Hallucination-Aware Retrieval-Augmented Generation (RAG)
            1. IDEA: Design a feedback loop-augmented RAG system that integrates hallucination detection and correction during generation. The model validates generated content against a trusted external knowledge base, iteratively refining responses.
            2. IMPLEMENTATION:
                - Knowledge-Enhanced RAG: Combine LLMs with real-time retrieval systems (e.g., semantic search) to fetch relevant knowledge dynamically.
                - Hallucination Detection Module: Add a discriminator or factual consistency checker trained to detect hallucinations by comparing generated content with retrieved data.
                - Refinement Loop: If inconsistencies are found, prompt the model to refine or regenerate specific parts of the response iteratively.
            3. ALGORITHM:
                - Input Query -> Retrieve Context -> Generate Response -> Factual Consistency Check -> (If Failure) Regenerate -> Output.
</IMPORTANT>
